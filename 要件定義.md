# ✅ Otrio AI開発：やるべきことリスト（AlphaZero方式）

---

## 🧩 1. ゲーム環境の構築

* [ ] 盤面状態の定義（3×3×3 or フラットな表現）
* [ ] 合法手の生成（指せるマス・サイズ・色の組み合わせ）
* [ ] 指し手の適用と状態遷移
* [ ] 勝敗・引き分け判定ロジック
* [ ] 対戦ログの出力形式（勝敗・手順記録）

---

## 🌲 2. MCTS（モンテカルロ木探索）

* [ ] 木構造のノードクラス定義（状態・Q/N・P・親子関係）
* [ ] 探索パラメータ（探索回数、温度、C\_PUCTなど）設定
* [ ] 選択（Select）処理（UCT式で子ノード選択）
* [ ] 拡張（Expand）処理（未展開手をノードとして追加）
* [ ] 評価（Evaluate）処理（NNからpolicy/valueを取得）
* [ ] バックアップ（Backup）処理（Q/Nを更新）
* [ ] 最終手の決定処理（最大訪問数など）

---

## 🧠 3. ニューラルネットワーク（Policy + Value）

* [ ] 入力形式の定義（盤面を多チャネルTensorに変換）
* [ ] 出力：

  * policy：合法手の確率分布（サイズ = 行動数）
  * value：現在の状態の勝率推定（-1〜1）
* [ ] ネットワーク構成（PyTorchやTensorFlowで実装）
* [ ] 損失関数の定義：

  * policyのクロスエントロピー
  * valueの平均二乗誤差（MSE）
* [ ] Optimizer・学習率の設定
* [ ] モデルの保存・読み込み処理

---

## 🤖 4. 自己対戦による学習ループ

* [ ] 1局の自己対戦実行（MCTSでプレイ）
* [ ] 各局面ごとに：

  * 状態
  * MCTSで得たpolicy（訪問回数から算出）
  * 結果（勝ち=+1、負け=-1、引き分け=0）
    を保存
* [ ] Replay Buffer（プレイデータ保存構造）の定義
* [ ] 学習データのシャッフル＆ミニバッチ処理
* [ ] ニューラルネットのトレーニング実行

---

## 🆚 5. モデルの評価と選別

* [ ] 旧モデル vs 新モデルのAI対戦を複数局実施
* [ ] 勝率を元に新モデル採用の可否を判定
* [ ] 勝率ログやモデル比較結果の記録

---

## 📦 6. その他の補助機能・管理

* [ ] ハイパーパラメータ設定用のconfigファイル（YAML/JSONなど）
* [ ] ログ記録と可視化（勝率推移・損失グラフなど）
* [ ] 実行CLI or Web UI（streamlitなど）で人間との対戦機能（任意）
* [ ] テストケースの作成（ゲームロジック・MCTS・NNの単体検証）

---

## ✨ 追加で検討できる項目（オプション）

* [ ] Data Augmentation（回転・反転によるデータ拡張）
* [ ] 複数プレイヤー対応（2人以外のルール対応）
* [ ] GPU活用で高速化（NN推論・学習の高速化）
* [ ] Webベース対戦システム（Flask + JS、streamlitなど）