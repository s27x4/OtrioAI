# ✅ Otrio AI開発：やるべきことリスト（AlphaZero方式）

---

## 🧩 1. ゲーム環境の構築

* [x] 盤面状態の定義（3×3×3 or フラットな表現）
* [x] 合法手の生成（指せるマス・サイズ・色の組み合わせ）
* [x] 指し手の適用と状態遷移
* [x] 勝敗・引き分け判定ロジック
* [x] 対戦ログの出力形式（勝敗・手順記録）

---

## 🌲 2. MCTS（モンテカルロ木探索）

* [x] 木構造のノードクラス定義（状態・Q/N・P・親子関係）
* [x] 探索パラメータ（探索回数、温度、C\_PUCTなど）設定
* [x] 選択（Select）処理（UCT式で子ノード選択）
* [x] 拡張（Expand）処理（未展開手をノードとして追加）
* [x] 評価（Evaluate）処理（NNからpolicy/valueを取得）
* [x] バックアップ（Backup）処理（Q/Nを更新）
* [x] 最終手の決定処理（最大訪問数など）

---

## 🧠 3. ニューラルネットワーク（Policy + Value）

* [x] 入力形式の定義（盤面を多チャネルTensorに変換）
* [x] 出力：

  * policy：合法手の確率分布（サイズ = 行動数）
  * value：現在の状態の勝率推定（-1〜1）
* [x] ネットワーク構成（PyTorchやTensorFlowで実装）
* [x] 損失関数の定義：

  * policyのクロスエントロピー
  * valueの平均二乗誤差（MSE）
* [x] Optimizer・学習率の設定
* [x] モデルの保存・読み込み処理

---

## 🤖 4. 自己対戦による学習ループ

* [x] 1局の自己対戦実行（MCTSでプレイ）
* [x] 各局面ごとに：

  * 状態
  * MCTSで得たpolicy（訪問回数から算出）
  * 結果（勝ち=+1、負け=-1、引き分け=0）
    を保存
* [x] Replay Buffer（プレイデータ保存構造）の定義
* [x] 学習データのシャッフル＆ミニバッチ処理
* [x] ニューラルネットのトレーニング実行

---

## 🆚 5. モデルの評価と選別

* [x] 旧モデル vs 新モデルのAI対戦を複数局実施
* [x] 勝率を元に新モデル採用の可否を判定
* [x] 勝率ログやモデル比較結果の記録

---

## 📦 6. その他の補助機能・管理

* [x] ハイパーパラメータ設定用のconfigファイル（YAML/JSONなど）
* [x] ログ記録と可視化（勝率推移・損失グラフなど）
* [x] 実行CLI or Web UI（streamlitなど）で人間との対戦機能（任意）
* [x] テストケースの作成（ゲームロジック・MCTS・NNの単体検証）

---

## ✨ 追加で検討できる項目（オプション）

* [x] Data Augmentation（回転・反転によるデータ拡張）
* [x] 複数プレイヤー対応（2人以外のルール対応）
* [x] GPU活用で高速化（NN推論・学習の高速化）
* [x] Webベース対戦システム（Flask + JS、streamlitなど）